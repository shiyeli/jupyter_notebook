{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from tensorflow_6_4_LeNet5_mnist_inference.ipynb\n",
      "Extracting ./MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting ./MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting ./MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "After 1 training steps,loss=5.535452\n",
      "After 1001 training steps,loss=90.634979\n",
      "After 2001 training steps,loss=77.901741\n",
      "After 3001 training steps,loss=67.195747\n",
      "After 4001 training steps,loss=58.142994\n",
      "After 5001 training steps,loss=50.490593\n",
      "After 6001 training steps,loss=43.999107\n",
      "After 7001 training steps,loss=38.479645\n",
      "After 8001 training steps,loss=33.789318\n",
      "After 9001 training steps,loss=29.729050\n",
      "After 10001 training steps,loss=26.286127\n"
     ]
    }
   ],
   "source": [
    "import ipynb_importer\n",
    "import tensorflow_6_4_LeNet5_mnist_inference as mnist_inference\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import numpy as np\n",
    "\n",
    "BATCH_SIZE=100\n",
    "LEARNING_RATE_BASE=0.8\n",
    "LEARNING_RATE_DECAY=0.99\n",
    "REGULARAZTION_RATE=0.0001\n",
    "TRAINING_STEPS=30000\n",
    "MOVING_AVERAGE_DECAY=0.99\n",
    "\n",
    "MODEL_SAVE_PATH='./tmp/LeNet5/'\n",
    "MODEL_NAME='model.ckpt'\n",
    "\n",
    "def train(mnist):\n",
    "    x=tf.placeholder(\n",
    "        tf.float32,\n",
    "        [None,mnist_inference.IMAGE_SIZE,mnist_inference.IMAGE_SIZE,mnist_inference.NUM_CHANNELS],\n",
    "        name='x-input'\n",
    "    )\n",
    "    y_=tf.placeholder(tf.float32,[None,mnist_inference.OUTPUT_NODE],name='y-input')\n",
    "    \n",
    "    regularizer=tf.contrib.layers.l2_regularizer(REGULARAZTION_RATE)\n",
    "    \n",
    "    #向前传播\n",
    "    y=mnist_inference.inference(x,True,regularizer)\n",
    "    global_step=tf.Variable(0,trainable=False)\n",
    "    \n",
    "    variable_averages=tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY,global_step)\n",
    "    variable_averages_op=variable_averages.apply(tf.trainable_variables())\n",
    "    \n",
    "    cross_entropy=tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y,labels=tf.argmax(y_,1))\n",
    "    cross_entropy_mean=tf.reduce_mean(cross_entropy)\n",
    "    \n",
    "    loss=cross_entropy_mean+tf.add_n(tf.get_collection('losses'))\n",
    "    learning_rate=tf.train.exponential_decay(\n",
    "        LEARNING_RATE_BASE,\n",
    "        global_step,\n",
    "        mnist.train.num_examples/BATCH_SIZE,\n",
    "        LEARNING_RATE_DECAY\n",
    "    )\n",
    "    train_step=tf.train.GradientDescentOptimizer(learning_rate).minimize(loss,global_step=global_step)\n",
    "    \n",
    "    with tf.control_dependencies([train_step,variable_averages_op]):\n",
    "        train_op=tf.no_op(name='train')\n",
    "        \n",
    "    saver=tf.train.Saver()\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        for i in range(TRAINING_STEPS):\n",
    "            xs,ys=mnist.train.next_batch(BATCH_SIZE)\n",
    "            reshaped_xs=np.reshape(xs,[\n",
    "                BATCH_SIZE,\n",
    "                mnist_inference.IMAGE_SIZE,\n",
    "                mnist_inference.IMAGE_SIZE,\n",
    "                mnist_inference.NUM_CHANNELS\n",
    "            ])\n",
    "            _,loss_value,step=sess.run([train_op,loss,global_step],feed_dict={x:reshaped_xs,y_:ys})\n",
    "            \n",
    "            #每1000轮保存一次模型\n",
    "            if i % 1000 ==0:\n",
    "                print 'After %d training steps,loss=%f' % (step,loss_value)\n",
    "                \n",
    "                #保存模型\n",
    "                saver.save(sess,os.path.join(MODEL_SAVE_PATH,MODEL_NAME),global_step=step)\n",
    "                \n",
    "                \n",
    "def main(argv=None):\n",
    "    mnist=input_data.read_data_sets('./MNIST_data',one_hot=True)\n",
    "    train(mnist)\n",
    "    \n",
    "if __name__=='__main__':\n",
    "    \"\"\"\n",
    "    ValueError: Variable layer1/weights already exists, disallowed. \n",
    "    Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:\n",
    "    第一次运行将下行代码注释，以后运行放开\n",
    "    \"\"\"\n",
    "#     tf.get_variable_scope().reuse_variables()\n",
    "    \n",
    "    tf.app.run()\n",
    "            \n",
    "         "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
